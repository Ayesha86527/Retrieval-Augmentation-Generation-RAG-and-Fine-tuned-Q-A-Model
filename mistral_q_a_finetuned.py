# -*- coding: utf-8 -*-
"""Mistral-Q/A Finetuned.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18Q9nRaBYMqI5PhJmCSuYxHdJ29rTo7VI
"""

!pip install torch transformers peft datasets
!pip install -U bitsandbytes

from huggingface_hub import notebook_login
notebook_login()

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model
from datasets import load_dataset

# QLoRA 4-bit config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
)

# Load 4-bit quantized model
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.3",
    quantization_config=bnb_config,
    device_map="auto"
)

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3")

# Dataset
data = load_dataset("rajpurkar/squad_v2")

import random

def format_with_citation(example):
    context = example["context"]
    question = example["question"]
    answer = example["answers"]["text"][0] if example["answers"]["text"] else "No answer"
    # Generating dummy citation
    chunk_id = random.randint(1, 100)
    page = random.randint(1, 50)
    timestamp = f"00:{random.randint(0,59):02}:{random.randint(0,59):02}"

    answer_with_citation = f"{answer} (chunk_{chunk_id}/page {page}/{timestamp})"

    input_text = f"""[INST] You are a helpful Q/A assistant.
Rules:
- Always answer in â‰¤3 sentences.
- Always include citation in this format: (chunk_id/page/timestamp).
- Never use "Answer:", "Short answer:", or bullet points.
- Output must be exactly one block of text: Answer + citation.

Here is the context:
```{context}```

Question: {question} [/INST]"""
    return {"input": input_text, "output": answer_with_citation}

subset = data["train"].select(range(5000))

# Split train/validation
split = subset.train_test_split(test_size=0.1, seed=42)

# Format rows
formatted_train = split["train"].map(format_with_citation)
formatted_val = split["test"].map(format_with_citation)

tokenizer.pad_token = tokenizer.eos_token
def tokenize_function(examples):
    # Tokenize input prompts
    inputs = tokenizer(
        examples["input"],
        truncation=True,
        padding="max_length",  # ensures tensors align
        max_length=128
    )

    # Tokenize outputs (answers)
    outputs = tokenizer(
        examples["output"],
        truncation=True,
        padding="max_length",
        max_length=128
    )

    # Labels = output_ids
    inputs["labels"] = outputs["input_ids"]
    return inputs

# Apply to dataset
tokenized_train = formatted_train.map(tokenize_function, batched=True, remove_columns=formatted_train.column_names)
tokenized_val = formatted_val.map(tokenize_function, batched=True, remove_columns=formatted_val.column_names)

# Add before trainer initialization
import gc
torch.cuda.empty_cache()
gc.collect()

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)
model = get_peft_model(model, peft_config)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

training_args = TrainingArguments(
    output_dir="outputs",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=1e-4,
    num_train_epochs=1,
    warmup_ratio=0.03,
    fp16=True,
    logging_steps=10,
    eval_strategy="no",
    save_strategy="epoch",
    save_total_limit=1,
    dataloader_pin_memory=True,
    report_to="none",
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    data_collator=data_collator,
)

model.config.use_cache = False
trainer.train()

# Save adapter
trainer.save_model("./mistral-7b-qlora-adapter-Q/A")
tokenizer.save_pretrained("./mistral-7b-qlora-adapter-Q/A")

# Save merged model
merged_model = model.merge_and_unload()
merged_model.save_pretrained("./mistral-7b-qlora-merged-Q/A")
tokenizer.save_pretrained("./mistral-7b-qlora-merged-Q/A")

print(" Training complete! Models saved:")

from google.colab import drive
drive.mount('/content/drive')
model.save_pretrained("/content/drive/MyDrive/mistral-7b-qlora-merged-Q/A")
tokenizer.save_pretrained("/content/drive/MyDrive/mistral-7b-qlora-merged-Q/A")

test_dataset = load_dataset("rajpurkar/squad_v2", split="validation[:50]")

formatted_test_dataset = test_dataset.map(format_with_citation)

from transformers import pipeline

qa_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=128,
    do_sample=False,
    temperature=0.0
)

import re
import time

def has_citation(text):
    """Check if citation is present."""
    return bool(re.search(r"\(chunk.*?\)", text))

def count_sentences(text):
    """Count sentences (approx using . ? !)"""
    return len(re.findall(r"[.!?]", text))

def f1_score(prediction, ground_truth):
    """Compute F1 overlap between words."""
    pred_tokens = prediction.lower().split()
    gt_tokens = ground_truth.lower().split()
    common = set(pred_tokens) & set(gt_tokens)
    if len(common) == 0:
        return 0.0
    precision = len(common) / len(pred_tokens)
    recall = len(common) / len(gt_tokens)
    return 2 * precision * recall / (precision + recall)

def exact_match(prediction, ground_truth):
    return int(prediction.strip().lower() == ground_truth.strip().lower())

import pandas as pd

results = []

for row in formatted_test_dataset:
    prompt = row["input"]
    gold_answer = row["output"]

    start = time.time()
    output = qa_pipeline(prompt, max_new_tokens=128, do_sample=False)[0]["generated_text"]
    latency = time.time() - start

    # Extract model's final answer
    answer = output.split("[/INST]")[-1].strip()

    # Checks
    citation_ok = has_citation(answer)
    em = exact_match(answer, gold_answer)
    f1 = f1_score(answer, gold_answer)
    length_ok = count_sentences(answer) <= 3

    results.append({
        "question": row["input"][:80] + "...",
        "gold": gold_answer,
        "pred": answer,
        "EM": em,
        "F1": round(f1, 3),
        "Citation": citation_ok,
        "Len<=3": length_ok,
        "Latency(s)": round(latency, 2),
    })

df = pd.DataFrame(results)

# Aggregate metrics
print("==== Aggregate Results ====")
print("Exact Match:", df["EM"].mean())
print("F1:", df["F1"].mean())
print("Citation compliance:", df["Citation"].mean())
print("Length compliance:", df["Len<=3"].mean())
print("Avg Latency:", df["Latency(s)"].mean())